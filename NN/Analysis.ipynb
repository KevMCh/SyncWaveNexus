{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713cfbc5",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "Import the required libraries for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ccbd2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    FILE_PATH = ''\n",
    "    \n",
    "    def __init__(self, user_id=None, label_list=None, num_samples=None, num_steps=None):\n",
    "        self.user_id = user_id\n",
    "        self.label_list = label_list\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.data, self.labels = self.load_data(user_id, label_list, num_steps)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.data[idx]), self.labels[idx]\n",
    "    \n",
    "    def load_data(self, user_id, label_list, num_steps):\n",
    "        data = list()\n",
    "        labels = list()\n",
    "        \n",
    "        for i in range(len(label_list)):\n",
    "            for j in range(1, self.num_samples + 1):\n",
    "                try:\n",
    "                    adjMatrix = scipy.io.loadmat(\n",
    "                        self.FILE_PATH + str(user_id) +\n",
    "                        '_label' + str(label_list[i]) +\n",
    "                        '_item' + str(j) +\n",
    "                        '_steps' + str(num_steps) + '.mat')\n",
    "                                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Load fail: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "                if num_steps == 1:\n",
    "                    tmpAdjMatrix = adjMatrix['fullMatrix'][np.newaxis, :, :]\n",
    "                else:\n",
    "                    tmpAdjMatrix = list()\n",
    "                    for k in range(num_steps):\n",
    "                        tmpAdjMatrix.append(adjMatrix['fullMatrix'][:, :, k])\n",
    "                                        \n",
    "                data.append(np.array(tmpAdjMatrix))\n",
    "                labels.append(label_list[i])\n",
    "                                \n",
    "        if not data or not labels:\n",
    "            raise RuntimeError(\"No data or labels loaded.\")\n",
    "                    \n",
    "        data = torch.tensor(np.stack(data), dtype=torch.float32)\n",
    "                \n",
    "        label_dict = {label: index for index, label in enumerate(self.label_list)}\n",
    "        new_label_list = [label_dict[label] for label in labels]\n",
    "        labels = torch.tensor(new_label_list, dtype=torch.long)\n",
    "        \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ecf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(full_dataset, batch_size, perc):\n",
    "    train_size = int(perc * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca92c6c",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e48da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMIAged(nn.Module):\n",
    "    def __init__(self, label_list):\n",
    "        super(CNNMIAged, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.norm1 = nn.BatchNorm2d(6)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.norm2 = nn.BatchNorm2d(16)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.pool1 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=5)\n",
    "        self.norm3 = nn.BatchNorm2d(16)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16, len(label_list))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.norm1(self.conv1(x)))\n",
    "        \n",
    "        x = F.elu(self.norm2(self.conv2(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.elu(self.norm3(self.conv3(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f413e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_CNN(net, num_epochs, train_loader, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    \n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, targets = data\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    emissions: float = tracker.stop()\n",
    "            \n",
    "    return (time.time() - start_time) / 60, emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e153e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network_CNN(net, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf3ef8",
   "metadata": {},
   "source": [
    "### SCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.9\n",
    "\n",
    "class SCNN(nn.Module):\n",
    "    def __init__(self, label_list):\n",
    "        super(SCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.norm1 = nn.BatchNorm2d(6)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.norm2 = nn.BatchNorm2d(16)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.pool1 = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=5)\n",
    "        self.norm3 = nn.BatchNorm2d(16)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(144, len(label_list))\n",
    "        self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "        \n",
    "        cur1 = self.norm1(self.conv1(x))\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "        \n",
    "        cur2 = self.norm2(self.conv2(spk1))\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        x = self.dropout1(spk2)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        cur3 = self.norm3(self.conv3(x))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        cur4 = self.fc1(x)\n",
    "        spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "        return spk4, mem4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data):\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        spk_out, mem_out = net(data)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "        \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_SCNN(net, num_epochs, train_loader, lr, batch_size, num_steps):    \n",
    "    loss_fn = SF.ce_rate_loss()\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "        \n",
    "    tracker = EmissionsTracker()\n",
    "    tracker.start()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for data, targets in iter(train_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            net.train()\n",
    "            spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()   \n",
    "    \n",
    "    emissions: float = tracker.stop()\n",
    "            \n",
    "    return (time.time() - start_time) / 60, emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network_SCNN(net, test_loader, num_steps):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        for data, targets in iter(test_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "            correct += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447028cf",
   "metadata": {},
   "source": [
    "# Calculation of the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ef117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_printer(user_id):\n",
    "    print(\"\\n\" * 2)\n",
    "    print(\"#\" * 50)\n",
    "    print(\"#                     User \" + str(i) + \"                     #\")\n",
    "    print(\"#\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rept = 1\n",
    "num_users = 9\n",
    "\n",
    "num_samples = 72\n",
    "label_list = [769, 770]\n",
    "\n",
    "lr = 1e-2\n",
    "batch_size = 6\n",
    "num_epochs = 200\n",
    "\n",
    "# Dataloader arguments\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_inputs =  22 * 22\n",
    "num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_outcomes = {\n",
    "    'acc': [],\n",
    "    'duration': [],\n",
    "    'carbon': []\n",
    "}\n",
    "\n",
    "SCNN_outcomes = {\n",
    "    'acc': [],\n",
    "    'duration': [],\n",
    "    'carbon': []\n",
    "}\n",
    "\n",
    "for i in range(1, num_users + 1):\n",
    "    user_printer(i)\n",
    "    \n",
    "    user_acc_CNN = []\n",
    "    user_duration_CNN = []\n",
    "    user_carbon_CNN = []\n",
    "    \n",
    "    user_acc_SCNN = []\n",
    "    user_duration_SCNN = []\n",
    "    user_carbon_SCNN = []\n",
    "    \n",
    "    ## Load the dataset of a specific user\n",
    "    full_dataset = GraphDataset(user_id=i, label_list=label_list,\n",
    "                            num_samples=num_samples, num_steps=num_steps)\n",
    "    \n",
    "    for j in range(rept):\n",
    "        train_loader, test_loader = split_dataset(full_dataset, batch_size, 0.75)\n",
    "        \n",
    "        ## Classification        \n",
    "        # Load the network\n",
    "        CNN_net = CNNMIAged(label_list).to(device)\n",
    "        SCNN_net = SCNN(label_list).to(device)\n",
    "        \n",
    "        ### Training\n",
    "        ## CNN\n",
    "        duration, carbon = training_network_CNN(CNN_net, num_epochs, train_loader, lr)\n",
    "        user_duration_CNN.append(duration)\n",
    "        user_carbon_CNN.append(carbon)\n",
    "        \n",
    "        acc = test_network_CNN(CNN_net, test_loader)\n",
    "        user_acc_CNN.append(acc)\n",
    "        \n",
    "        ## SCNN\n",
    "        duration, carbon = training_network_SCNN(SCNN_net, num_epochs, train_loader, lr, batch_size, num_steps)\n",
    "        user_duration_SCNN.append(duration)\n",
    "        user_carbon_SCNN.append(carbon)\n",
    "        \n",
    "        acc = test_network_SCNN(SCNN_net, test_loader, num_steps)\n",
    "        user_acc_SCNN.append(acc)\n",
    "        \n",
    "    CNN_outcomes['acc'].append(user_acc_CNN)\n",
    "    CNN_outcomes['duration'].append(user_duration_CNN)\n",
    "    CNN_outcomes['carbon'].append(user_carbon_CNN)\n",
    "    \n",
    "    SCNN_outcomes['acc'].append(user_acc_SCNN)\n",
    "    SCNN_outcomes['duration'].append(user_duration_SCNN)\n",
    "    SCNN_outcomes['carbon'].append(user_carbon_SCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_statistics(metrics):\n",
    "    values = np.array(metrics)\n",
    "    mean = np.mean(values, axis=1)\n",
    "    std = np.std(values, axis=1)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_statistics(CNN_outcomes['acc']))\n",
    "print(calc_statistics(CNN_outcomes['duration']))\n",
    "print(calc_statistics(CNN_outcomes['carbon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cbbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_statistics(SCNN_outcomes['acc']))\n",
    "print(calc_statistics(SCNN_outcomes['duration']))\n",
    "print(calc_statistics(SCNN_outcomes['carbon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a10598",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCNN_time_outcomes = {\n",
    "    'acc': [],\n",
    "    'duration': [],\n",
    "    'carbon': []\n",
    "}\n",
    "\n",
    "for i in range(1, num_users + 1):\n",
    "    user_printer(i)\n",
    "    \n",
    "    user_acc_SCNN = []\n",
    "    user_duration_SCNN = []\n",
    "    user_carbon_SCNN = []\n",
    "        \n",
    "    ## Load the dataset of a specific user\n",
    "    full_dataset = GraphDataset(user_id=i, label_list=label_list,\n",
    "                            num_samples=num_samples, num_steps=1)\n",
    "    \n",
    "    for j in range(rept):\n",
    "        train_loader, test_loader = split_dataset(full_dataset, batch_size, 0.75)\n",
    "        \n",
    "        ## Classification\n",
    "        # Load the network\n",
    "        SCNN_net = SCNN(label_list).to(device)\n",
    "        \n",
    "        ### Training\n",
    "        ## SCNN\n",
    "        duration, carbon = training_network_SCNN(SCNN_net, num_epochs, train_loader, lr, batch_size, num_steps)\n",
    "        user_duration_SCNN.append(duration)\n",
    "        user_carbon_SCNN.append(carbon)\n",
    "        \n",
    "        acc = test_network_SCNN(SCNN_net, test_loader, num_steps)\n",
    "        user_acc_SCNN.append(acc)\n",
    "        \n",
    "    SCNN_time_outcomes['acc'].append(user_acc_SCNN)\n",
    "    SCNN_time_outcomes['duration'].append(user_duration_SCNN)\n",
    "    SCNN_time_outcomes['carbon'].append(user_carbon_SCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54858627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_statistics(SCNN_time_outcomes['acc']))\n",
    "print(calc_statistics(SCNN_time_outcomes['duration']))\n",
    "print(calc_statistics(SCNN_time_outcomes['carbon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d3ee6",
   "metadata": {},
   "source": [
    "#### Time evolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data):\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        data_step = data[:, step, :, :].unsqueeze(1)\n",
    "        \n",
    "        spk_out, mem_out = net(data_step)\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "        \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCNN_evol_time_outcomes = {\n",
    "    'acc': [],\n",
    "    'duration': [],\n",
    "    'carbon': []\n",
    "}\n",
    "\n",
    "for i in range(1, num_users + 1):\n",
    "    user_printer(i)\n",
    "    \n",
    "    user_acc_SCNN = []\n",
    "    user_duration_SCNN = []\n",
    "    user_carbon_SCNN = []\n",
    "    \n",
    "    ## Load the dataset of a specific user\n",
    "    full_dataset = GraphDataset(user_id=i, label_list=label_list,\n",
    "                            num_samples=num_samples, num_steps=num_steps)\n",
    "    \n",
    "    for j in range(rept):\n",
    "        train_loader, test_loader = split_dataset(full_dataset, batch_size, 0.75)\n",
    "        \n",
    "        ## Classification\n",
    "        # Load the network\n",
    "        SCNN_net = SCNN(label_list).to(device)\n",
    "        \n",
    "        ### Training\n",
    "        ## SCNN\n",
    "        duration, carbon = training_network_SCNN(SCNN_net, num_epochs, train_loader, lr, batch_size, num_steps)\n",
    "        user_duration_SCNN.append(duration)\n",
    "        user_carbon_SCNN.append(carbon)\n",
    "        \n",
    "        acc = test_network_SCNN(SCNN_net, test_loader, num_steps)\n",
    "        user_acc_SCNN.append(acc)\n",
    "        \n",
    "    SCNN_evol_time_outcomes['acc'].append(user_acc_SCNN)\n",
    "    SCNN_evol_time_outcomes['duration'].append(user_duration_SCNN)\n",
    "    SCNN_evol_time_outcomes['carbon'].append(user_carbon_SCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3285cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_statistics(SCNN_evol_time_outcomes['acc']))\n",
    "print(calc_statistics(SCNN_evol_time_outcomes['duration']))\n",
    "print(calc_statistics(SCNN_evol_time_outcomes['carbon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c80a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
